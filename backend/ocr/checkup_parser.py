import gc
import json
import re
import os
import subprocess
from datetime import datetime
from bs4 import BeautifulSoup
from collections import defaultdict
from typing import Any, Dict, List, Tuple, Optional, Union
# from llama_cpp import Llama
from transformers import AutoModelForCausalLM, AutoTokenizer
from vllm import LLM, SamplingParams
from backend.settings import settings


def parse_ocr_result(ocr_output: list) -> Dict[str, Any]:
    if not ocr_output:
        raise ValueError("OCRè¾“å‡ºä¸ºç©º")

    # å•å›¾å¤„ç†
    page_data = ocr_output[0].get("res", ocr_output[0])
    blocks = page_data.get("parsing_res_list", [])

    text_blocks = []
    table_blocks = []
    for block in blocks:
        if block.get("block_label") == "text":
            text_blocks.append(block)
        elif block.get("block_label") == "table":
            table_blocks.append(block)

    tables_data = []
    for idx, table_block in enumerate(table_blocks):
        try:
            table_html = table_block.get("block_content")
            table_html = table_html_clean(table_html)
            table_md = table_html_to_md(table_html)
            tables_data.append(table_md_to_json(table_md))
        except Exception as e:
            print(f"âš ï¸ è¡¨æ ¼å— {idx} è§£æå¤±è´¥: {str(e)}")
            raise

    full_text = "\n".join([b["block_content"] for b in text_blocks])

    return {
        "tables": tables_data,
        "full_text": full_text,
        "stats": {
            "table_blocks": len(table_blocks),
            "parsed_tables": len(tables_data),
            "text_blocks": len(text_blocks),
        }
    }


def table_html_clean(table_html: str) -> str:
    """
    æ¸…æ´—HTMLä¸­çš„è½¬ä¹‰å­—ç¬¦ï¼Œå°†å…¶è½¬æ¢ä¸ºæ­£å¸¸å­—ç¬¦
    """
    if not table_html:
        return table_html

    # å®šä¹‰è½¬ä¹‰å­—ç¬¦æ˜ å°„è¡¨
    escape_map = {
        r'\\uparrow ': 'â†‘ ',      # ä¸Šç®­å¤´
        r'\\downarrow ': 'â†“ ',    # ä¸‹ç®­å¤´
        r'\\times ': 'Ã— ',        # ä¹˜å·
        r'\\mu ': 'Î¼ ',           # åˆ é™¤\m
    }

    # é€ä¸ªæ›¿æ¢è½¬ä¹‰å­—ç¬¦
    cleaned_html = table_html
    for escape_seq, normal_char in escape_map.items():
        cleaned_html = cleaned_html.replace(escape_seq, normal_char)

    return cleaned_html


def table_html_to_md(table_html: str) -> str:
    soup = BeautifulSoup(table_html, 'lxml')
    table = soup.find('table')

    if not table or not table.find('tr'):
        return None

    md_lines = []
    header_row = table.find('tr')
    raw_headers = [th.get_text(strip=True)
                   for th in header_row.find_all(['td', 'th'])]

    # æ ‡å‡†åŒ–è¡¨å¤´åˆ«åï¼ˆå…³é”®ï¼ç»Ÿä¸€åç»­åˆ¤æ–­åŸºå‡†ï¼‰
    header_map = {
        'é¡¹ç›®': 'é¡¹ç›®åç§°', 'æ£€éªŒé¡¹ç›®': 'é¡¹ç›®åç§°', 'æŒ‡æ ‡': 'é¡¹ç›®åç§°', 'æ£€æŸ¥é¡¹ç›®': 'é¡¹ç›®åç§°',
        'ç»“æœ': 'æ£€æŸ¥ç»“æœ', 'æµ‹å®šå€¼': 'æ£€æŸ¥ç»“æœ', 'å®æµ‹å€¼': 'æ£€æŸ¥ç»“æœ',
        'å‚è€ƒèŒƒå›´': 'å‚è€ƒå€¼', 'æ­£å¸¸å€¼': 'å‚è€ƒå€¼', 'å‚è€ƒåŒºé—´': 'å‚è€ƒå€¼',
        'å•ä½': 'å•ä½', 'è®¡é‡å•ä½': 'å•ä½'
    }
    headers = [header_map.get(h, h) for h in raw_headers]

    rows = []
    for tr in table.find_all('tr')[1:]:
        cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
        # row_dict = dict(zip(headers, cells))
        rows.append(cells)

    md_lines.append('| ' + ' | '.join(headers) + ' |')
    md_lines.append('| ' + ' | '.join([' --- ' for _ in headers]) + ' |')
    for row in rows:
        md_lines.append('| ' + ' | '.join(row) + ' |')

    md = '\n'.join(md_lines)

    return md


def table_md_to_json(table_md: str) -> Union[dict, list, None]:
    model_path = "/root/autodl-tmp/models/Qwen/Qwen3-8B-AWQ"

    llm = LLM(
        model=model_path,
        dtype="float16",
        quantization="awq",
        # gpu_memory_utilization=0.8,
        max_model_len=16384,
        enforce_eager=False,
        trust_remote_code=True,
    )
    tokenizer = llm.get_tokenizer()

    prompt = """
# Role
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ•°æ®ç»“æ„åŒ–åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ä½“æ£€æŠ¥å‘Šçš„ OCR è¯†åˆ«ç»“æœï¼ˆMarkdown æ ¼å¼ï¼‰ä¸­æå–å…³é”®æŒ‡æ ‡ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†çš„ JSON æ ¼å¼ã€‚
# Rules
1. **ä¸¥æ ¼è¾“å‡º JSON**ï¼šåªè¿”å› JSON ä»£ç å—ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€Markdown æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ç­‰å†…å®¹ã€‚
2. **å™ªéŸ³è¿‡æ»¤**ï¼šè¯·å¿½ç•¥æ‰€æœ‰éæ£€æŸ¥æ•°æ®çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
   - æŠ¥å‘Šæ ‡é¢˜ã€åŒ»é™¢åç§°ã€ä½“æ£€å·ã€æ¡å½¢ç ã€‚
   - é¡µçœ‰ã€é¡µç ã€æ‰“å°æ—¶é—´ã€‚
   - å®¡æ ¸åŒ»ç”Ÿã€æ£€éªŒè€…ã€æŠ¥å‘Šæ—¥æœŸã€é€æ£€æ—¥æœŸç­‰ã€‚
3. **è¯­ä¹‰æ¨æ–­**ï¼š
   - å¦‚æœæå–åˆ°çš„æ–‡æœ¬ä¸­åŒ…å«ç®­å¤´ç¬¦å·ï¼ˆâ†‘ â†“ + -ï¼‰ï¼Œå°†å…¶æå–åˆ° `is_abnormal` å­—æ®µä¸­ã€‚
   - å¦‚æœæ•°æ®æ²¡æœ‰å•ä½ï¼Œ`unit` å¡« nullã€‚
# Output Schema
è¯·è¾“å‡ºä¸€ä¸ªåŒ…å«ä»¥ä¸‹ç»“æ„çš„ JSON åˆ—è¡¨ï¼š
[
  {
    "item_name": "æ£€æŸ¥é¡¹ç›®åç§° (å¦‚: ç™½ç»†èƒ)",
    "result": "æ£€æŸ¥ç»“æœ (å­—ç¬¦ä¸²ï¼Œä¿ç•™åŸå§‹ç¬¦å·)",
    "unit": "å•ä½ (å¦‚: g/Lï¼Œè‹¥æ— åˆ™ä¸º null)",
    "reference_range": "å‚è€ƒèŒƒå›´ (å¦‚: 3.5-9.5ï¼Œè‹¥æ— åˆ™ä¸ºnull)",
    "is_abnormal": "å¼‚å¸¸æ ‡è®° (å¦‚æœ‰ç®­å¤´åˆ™ä¿ç•™ç®­å¤´å­—ç¬¦ï¼Œå¦‚ 'â†‘'ï¼Œå¦åˆ™ä¸º null)"
  }
]
# Examples
## Example 1 (æ ‡å‡†å•æ è¡¨æ ¼ï¼ŒåŒ…æ‹¬å¤§éƒ¨åˆ†æ­£å¸¸æƒ…å†µ)
### Input Markdown:
| é¡¹ç›®åç§° | æ£€æŸ¥ç»“æœ | å•ä½ | å‚è€ƒèŒƒå›´ |
|---|---|---|---|
| çº¢ç»†èƒ | â†‘ 6 | 10^12/L | 4.0-5.5 |
| è¡€çº¢è›‹ç™½ | 135 | g/L | 120-160 |
| é€æ˜åº¦ | é€æ˜ |  |  |
### Output JSON:
[
  {"item_name": "çº¢ç»†èƒ", "result": "6", "unit": "10^12/L", "reference_range": "4.0-5.5", "is_abnormal": "â†‘"},
  {"item_name": "è¡€çº¢è›‹ç™½", "result": "135", "unit": "g/L", "reference_range": "120-160", "is_abnormal": null},
  {"item_name": "é€æ˜åº¦", "result": "é€æ˜", "unit": null, "reference_range": null, "is_abnormal": null}
]
## Example 2 (åŒ…å«å™ªéŸ³çš„è„æ•°æ®)
### Input Markdown:
| é¡¹ç›®åç§° | æ£€æŸ¥ç»“æœ | å•ä½ | å‚è€ƒèŒƒå›´ |
|---|---|---|---|
| è›‹ç™½è´¨ | â†‘ 45 | U/L | 0-40 |
| ç™½ç»†èƒ | 28 | U/L | 0-40 |
| å®¡æ ¸åŒ»ç”Ÿï¼šå¼ ä¸‰ |  | æŠ¥å‘Šæ—¥æœŸï¼š2023-10-22 14:00:00 |  |
| ç¬¬ 1 é¡µ / å…± 2 é¡µ |  |  |  |
### Output JSON:
[
  {"item_name": "è›‹ç™½è´¨", "result": "45", "unit": "U/L", "reference_range": "0-40", "is_abnormal": "â†‘"},
  {"item_name": "ç™½ç»†èƒ", "result": "28", "unit": "U/L", "reference_range": "0-40", "is_abnormal": null}
]
## Example 3 (åŒæ è¡¨æ ¼)
### Input Markdown:
| é¡¹ç›®åç§° | æ£€æŸ¥ç»“æœ | å•ä½ | å‚è€ƒå€¼ | é¡¹ç›®åç§° | æ£€æŸ¥ç»“æœ | å•ä½ | å‚è€ƒå€¼ |
|  ---  |  ---  |  ---  |  ---  |  ---  |  ---  |  ---  |  ---  |
| å°¿èƒ†åŸ | é˜´æ€§ |  | é˜´æ€§ | ç»´ç”Ÿç´ C | 1.2 | mmol/L | 0.7-2.0 |
| è‘¡è„ç³– | é˜´æ€§ |  | é˜´æ€§ | é…¸ç¢±åº¦ | â†“ 4.2 |  | 4.5-8.0 |

### Output JSON:
[
  {"item_name": "å°¿èƒ†åŸ", "result": "é˜´æ€§", "unit": null, "reference_range": "é˜´æ€§", "is_abnormal": null},
  {"item_name": "ç»´ç”Ÿç´ C", "result": "1.2", "unit": "mmol/L", "reference_range": "0.7-2.0", "is_abnormal": null},
  {"item_name": "è‘¡è„ç³–", "result": "é˜´æ€§", "unit": null, "reference_range": "é˜´æ€§", "is_abnormal": null},
  {"item_name": "é…¸ç¢±åº¦", "result": "4.2", "unit": null, "reference_range": "4.5-8.0", "is_abnormal": "â†“"}
]
# Real Task
è¯·æ ¹æ®ä¸Šè¿°è§„åˆ™ï¼Œå¤„ç†ä»¥ä¸‹çœŸå®çš„ä½“æ£€æŠ¥å‘Š Markdown å†…å®¹ï¼š
    """
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": table_md}
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False
    )

    sampling_params = SamplingParams(
        temperature=0.7,
        top_k=20,
        top_p=0.8,
        max_tokens=8192,
        stop=["<|im_end|>", "<|endoftext|>"]  # è®¾ç½®åœæ­¢è¯
    )

    outputs = llm.generate([text], sampling_params)
    content = outputs[0].outputs[0].text

    print("æå–ç»“æœ:", content)
    parsed_result = safe_json_parse(content)

    if parsed_result is None:
        return []
    # elif isinstance(parsed_result, list):
    #     return {"items": parsed_result}
    else:
        return parsed_result


def safe_json_parse(text: str) -> Union[dict, list, None]:
    """
    å®‰å…¨åœ°è§£æJSONï¼ŒåŒ…å«å¤šç§æ¸…ç†ç­–ç•¥
    """
    if not text:
        return None

    # æ¸…ç†æ–‡æœ¬
    cleaned_text = text.strip()

    # ç§»é™¤å¸¸è§çš„å‰ç¼€/åç¼€
    prefixes_to_remove = ['```json', '```', 'json']
    suffixes_to_remove = ['```']

    for prefix in prefixes_to_remove:
        if cleaned_text.startswith(prefix):
            cleaned_text = cleaned_text[len(prefix):].strip()
            break

    for suffix in suffixes_to_remove:
        if cleaned_text.endswith(suffix):
            cleaned_text = cleaned_text[:-len(suffix)].strip()
            break

    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        pass

    # å°è¯•æå–æ•°ç»„éƒ¨åˆ†
    array_matches = re.findall(r'\[[\s\S]*?\]', cleaned_text)
    if array_matches:
        try:
            return json.loads(array_matches[-1])  # ä½¿ç”¨æœ€åä¸€ä¸ªåŒ¹é…çš„æ•°ç»„
        except json.JSONDecodeError:
            pass

    # å°è¯•æå–å¯¹è±¡éƒ¨åˆ†
    object_matches = re.findall(r'\{[\s\S]*?\}', cleaned_text)
    if object_matches:
        try:
            return json.loads(object_matches[-1])
        except json.JSONDecodeError:
            pass

    print(f"æ— æ³•è§£æJSON: {text[:100]}...")
    return None


def run_ocr(input_path):
    # 1. ä¸»ç¨‹åºè‡ªå·±å†³å®šè¾“å‡ºè·¯å¾„ï¼ˆæ¯”å¦‚å¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime(r"%Y%m%d_%H%M%S")
    output_json_path = settings.project_root / f"data/sensitive/ocr_output/{timestamp}/"

    # 2. è°ƒç”¨å­è¿›ç¨‹
    result = subprocess.run(
        [settings.ocr_python, settings.ocr_script, input_path, output_json_path],
        capture_output=True,
        text=True
    )

    # 3. æ£€æŸ¥æ˜¯å¦æˆåŠŸ
    # if result.returncode != 0:
    #     raise RuntimeError(f"OCR failed: {result.stderr.strip()}")

    # 4. è¿”å› JSON è·¯å¾„ï¼ˆä¸»ç¨‹åºå®Œå…¨æŒæ§ï¼‰
    return output_json_path


if __name__ == "__main__":
    output_json_path = run_ocr(settings.project_root / "tests/test_ocr/cam2/4.jpg")

    dates = []
    for filename in os.listdir(output_json_path):
        if filename.endswith('.json'):
            file_path = os.path.join(output_json_path, filename)

            # with open(file_path, 'r', encoding='utf-8') as f:
            #     data = json.load(f)
            #     json_data_list.append(data)
            dates.append(json.load(open(file_path)))
            print(f"æˆåŠŸè¯»å–: {filename}")
    # æ ¸å¿ƒï¼šè§£æç»“æ„åŒ–æ•°æ®
    try:
        structured_data = parse_ocr_result(dates)

        print("="*50)
        print(f"ğŸ“Š å…±è§£æ {structured_data['stats']['parsed_tables']} ä¸ªè¡¨æ ¼")
        print("="*50)

        # è¡¨æ ¼æ•°æ®ç¤ºä¾‹ï¼ˆä¾›åç»­æ ‡å‡†åŒ–/ç”»åƒç”Ÿæˆï¼‰
        if structured_data["tables"]:
            print("\nã€è¡¨æ ¼æ•°æ®ç¤ºä¾‹ã€‘")
            sample_row = structured_data["tables"][0][0]
            print(f"é¦–è¡Œ: {sample_row}")

        # æ–‡æœ¬æ®µè½ç¤ºä¾‹ï¼ˆä¾›NER/LLMå¤„ç†ï¼‰
        print("\nã€æ–‡æœ¬æ®µè½ã€‘")
        print(f"\n{structured_data['full_text']}\n")
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
