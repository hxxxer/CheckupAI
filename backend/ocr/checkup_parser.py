import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
from collections import defaultdict
from typing import Any, Dict, List, Tuple, Optional, Union
from backend.config import settings
from backend.instances import table_parser
from backend.ocr import PaddleOCRRunner


def parse_ocr_result(ocr_output: list) -> Dict[str, Any]:
    if not ocr_output:
        raise ValueError("OCRè¾“å‡ºä¸ºç©º")

    # å•å›¾å¤„ç†
    page_data = ocr_output[0].get("res", ocr_output[0])
    blocks = page_data.get("parsing_res_list", [])

    text_blocks = []
    table_blocks = []
    for block in blocks:
        if block.get("block_label") == "text":
            text_blocks.append(block)
        elif block.get("block_label") == "table":
            table_blocks.append(block)

    table_parser.wake_up()
    tables_data = []
    for idx, table_block in enumerate(table_blocks):
        try:
            table_html = table_block.get("block_content")
            table_html = table_html_clean(table_html)
            table_md = table_html_to_md(table_html)
            tables_data.append(table_parser.parse(table_md))
        except Exception as e:
            print(f"âš ï¸ è¡¨æ ¼å— {idx} è§£æå¤±è´¥: {str(e)}")
            raise
    table_parser.sleep()

    full_text = "\n".join([b["block_content"] for b in text_blocks])

    return {
        "tables": tables_data,
        "full_text": full_text,
        "stats": {
            "table_blocks": len(table_blocks),
            "parsed_tables": len(tables_data),
            "text_blocks": len(text_blocks),
        }
    }


def table_html_clean(table_html: str) -> str:
    """
    æ¸…æ´—HTMLä¸­çš„è½¬ä¹‰å­—ç¬¦ï¼Œå°†å…¶è½¬æ¢ä¸ºæ­£å¸¸å­—ç¬¦
    """
    if not table_html:
        return table_html

    # å®šä¹‰è½¬ä¹‰å­—ç¬¦æ˜ å°„è¡¨
    escape_map = {
        r'\\uparrow ': 'â†‘ ',      # ä¸Šç®­å¤´
        r'\\downarrow ': 'â†“ ',    # ä¸‹ç®­å¤´
        r'\\times ': ' Ã— ',        # ä¹˜å·
        r'\\mu ': 'Î¼',           # åˆ é™¤\m
    }

    # é€ä¸ªæ›¿æ¢è½¬ä¹‰å­—ç¬¦
    cleaned_html = table_html
    for escape_seq, normal_char in escape_map.items():
        cleaned_html = cleaned_html.replace(escape_seq, normal_char)

    return cleaned_html


def table_html_to_md(table_html: str) -> str:
    soup = BeautifulSoup(table_html, 'lxml')
    table = soup.find('table')

    if not table or not table.find('tr'):
        return None

    md_lines = []
    header_row = table.find('tr')
    raw_headers = [th.get_text(strip=True)
                   for th in header_row.find_all(['td', 'th'])]

    # æ ‡å‡†åŒ–è¡¨å¤´åˆ«åï¼ˆå…³é”®ï¼ç»Ÿä¸€åç»­åˆ¤æ–­åŸºå‡†ï¼‰
    header_map = {
        'é¡¹ç›®': 'é¡¹ç›®åç§°', 'æ£€éªŒé¡¹ç›®': 'é¡¹ç›®åç§°', 'æŒ‡æ ‡': 'é¡¹ç›®åç§°', 'æ£€æŸ¥é¡¹ç›®': 'é¡¹ç›®åç§°',
        'ç»“æœ': 'æ£€æŸ¥ç»“æœ', 'æµ‹å®šå€¼': 'æ£€æŸ¥ç»“æœ', 'å®æµ‹å€¼': 'æ£€æŸ¥ç»“æœ',
        'å‚è€ƒèŒƒå›´': 'å‚è€ƒå€¼', 'æ­£å¸¸å€¼': 'å‚è€ƒå€¼', 'å‚è€ƒåŒºé—´': 'å‚è€ƒå€¼',
        'å•ä½': 'å•ä½', 'è®¡é‡å•ä½': 'å•ä½'
    }
    headers = [header_map.get(h, h) for h in raw_headers]

    rows = []
    for tr in table.find_all('tr')[1:]:
        cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
        # row_dict = dict(zip(headers, cells))
        rows.append(cells)

    md_lines.append('| ' + ' | '.join(headers) + ' |')
    md_lines.append('| ' + ' | '.join([' --- ' for _ in headers]) + ' |')
    for row in rows:
        md_lines.append('| ' + ' | '.join(row) + ' |')

    md = '\n'.join(md_lines)

    return md


def run_ocr(input_path):
    """
    æ‰§è¡Œ OCR å¤„ç†
    
    Args:
        input_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
        
    Returns:
        è¾“å‡º JSON ç›®å½•è·¯å¾„
    """
    runner = PaddleOCRRunner()
    output_json_path = runner.run(input_path)
    return output_json_path


# æµ‹è¯•
if __name__ == "__main__":
    output_json_path = run_ocr(
        settings.project_root / "tests/test_ocr/cam2/4.jpg")

    dates = []
    for filename in os.listdir(output_json_path):
        if filename.endswith('.json'):
            file_path = os.path.join(output_json_path, filename)

            # with open(file_path, 'r', encoding='utf-8') as f:
            #     data = json.load(f)
            #     json_data_list.append(data)
            dates.append(json.load(open(file_path)))
            print(f"æˆåŠŸè¯»å–: {filename}")
    # æ ¸å¿ƒï¼šè§£æç»“æ„åŒ–æ•°æ®
    try:
        structured_data = parse_ocr_result(dates)

        print("="*50)
        print(f"ğŸ“Š å…±è§£æ {structured_data['stats']['parsed_tables']} ä¸ªè¡¨æ ¼")
        print("="*50)

        # è¡¨æ ¼æ•°æ®ç¤ºä¾‹ï¼ˆä¾›åç»­æ ‡å‡†åŒ–/ç”»åƒç”Ÿæˆï¼‰
        if structured_data["tables"]:
            print("\nã€è¡¨æ ¼æ•°æ®ç¤ºä¾‹ã€‘")
            sample_row = structured_data["tables"][0][0]
            print(f"é¦–è¡Œ: {sample_row}")

        # æ–‡æœ¬æ®µè½ç¤ºä¾‹ï¼ˆä¾›NER/LLMå¤„ç†ï¼‰
        print("\nã€æ–‡æœ¬æ®µè½ã€‘")
        print(f"\n{structured_data['full_text']}\n")
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
